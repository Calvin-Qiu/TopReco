#!/usr/bin/env python
"""
Training GNN
"""

import tensorflow as tf

import os
import sys
import argparse

import re
import time
import random
import functools
import six

import numpy as np
import sklearn.metrics


from graph_nets import utils_tf
from graph_nets import utils_np
import sonnet as snt

from root_gnn import model as all_models
from root_gnn import losses
# from root_gnn import optimizers
from root_gnn.src.datasets import graph
from root_gnn.utils import load_yaml

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train nx-graph with configurations')
    add_arg = parser.add_argument
    add_arg('config', help='configuration file')
    args = parser.parse_args()

    config = load_yaml(args.config)

    # add ops to save and restore all the variables
    prod_name = config['prod_name']
    output_dir = os.path.join(config['output_dir'], prod_name)
    os.makedirs(output_dir, exist_ok=True)

    config_tr = config['parameters']
    global_batch_size = n_graphs   = config_tr['batch_size']   # need optimization
    num_processing_steps_tr = config_tr['n_iters']      ## level of message-passing
    metric_name = config_tr['earlystop_metric']
    metric_dict = {
        "auc_te": 0.0, "acc_te": 0.0, "prec_te": 0.0, "rec_te": 0.0, "loss_te": 0.0
    }
    if metric_name not in metric_dict.keys():
        msg = "earlystop_metric: {} not supported. Use one of the following\n".format(metric_name) + "\n".join(list(metric_dict.keys()))
        raise ValueError(msg)
    acceptable_fails = config_tr['acceptable_failure']
    n_epochs = config_tr['epochs']  

 
    # prepare inputs
    tr_filenames = tf.io.gfile.glob(config['tfrec_dir_train'])
    n_train = len(tr_filenames)
    val_filenames = tf.io.gfile.glob(config['tfrec_dir_val'])
    n_val = len(val_filenames)

    shuffle_buffer_size = config_tr.get("shuffle_buffer_size", -1)
    AUTO = tf.data.experimental.AUTOTUNE
    # options = tf.data.Options()
    # options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
    # training_dataset = training_dataset.with_options(options)
    # training_dataset = training_dataset.batch(global_batch_size).prefetch(AUTO)

    training_dataset = tf.data.TFRecordDataset(tr_filenames)
    training_dataset = training_dataset.map(graph.parse_tfrec_function, num_parallel_calls=AUTO)
    n_train_graphs = sum([1 for _ in training_dataset])
    shuffle_train_size = n_train_graphs if shuffle_buffer_size < 0 else shuffle_buffer_size
    training_dataset = training_dataset.shuffle(shuffle_train_size, seed=12345, reshuffle_each_iteration=False)
    training_dataset = training_dataset.prefetch(AUTO)

    testing_dataset = tf.data.TFRecordDataset(val_filenames)
    testing_dataset = testing_dataset.map(graph.parse_tfrec_function, num_parallel_calls=AUTO)
    n_test_graphs = sum([1 for _ in testing_dataset])
    shuffle_eval_size = n_test_graphs if shuffle_buffer_size < 0 else shuffle_buffer_size
    testing_dataset = testing_dataset.shuffle(shuffle_eval_size, seed=12345, reshuffle_each_iteration=False)
    testing_dataset = testing_dataset.prefetch(AUTO)

    print("Input file names: ", tr_filenames)
    print("{} training files, {:,} Events".format(n_train, n_train_graphs))
    print("{} evaluation files, {:,} Events".format(n_val, n_test_graphs))
    print("Model saved at {}".format(output_dir))

    learning_rate = config_tr['learning_rate']
    # learning_rate = tf.compat.v1.train.exponential_decay(start_learning_rate, )
    optimizer = snt.optimizers.Adam(learning_rate)
    model = getattr(all_models, config['model_name'])()

    # inputs, targets = doublet_graphs.create_graph(batch_size)
    with_batch_dim = False
    input_list = []
    target_list = []
    for dd in training_dataset.take(global_batch_size).as_numpy_iterator():
        input_list.append(dd[0])
        target_list.append(dd[1])

    inputs = utils_tf.concat(input_list, axis=0)
    targets = utils_tf.concat(target_list, axis=0)
    input_signature = (
        graph.specs_from_graphs_tuple(inputs, with_batch_dim),
        graph.specs_from_graphs_tuple(targets, with_batch_dim)
    )

    loss_config = config['loss_name'].split(',')
    loss_name = loss_config[0]
    loss_fcn = getattr(losses, loss_config[0])(*[float(x) for x in loss_config[1:]])

    @functools.partial(tf.function, input_signature=input_signature)
    def update_step(inputs_tr, targets_tr):
        print("Tracing update_step")
        with tf.GradientTape() as tape:
            outputs_tr = model(inputs_tr, num_processing_steps_tr)
            loss_ops_tr = loss_fcn(targets_tr, outputs_tr)
            loss_op_tr = tf.math.reduce_sum(loss_ops_tr) / tf.constant(num_processing_steps_tr, dtype=tf.float32)

        gradients = tape.gradient(loss_op_tr, model.trainable_variables)
        optimizer.apply(gradients, model.trainable_variables)
        return outputs_tr, loss_op_tr

    time_stamp = time.strftime('%Y%m%d-%H%M%S', time.localtime())

    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
    ckpt_manager = tf.train.CheckpointManager(checkpoint, directory=output_dir,\
        max_to_keep=3, keep_checkpoint_every_n_hours=1)
    checkpoint.restore(ckpt_manager.latest_checkpoint)
    if ckpt_manager.latest_checkpoint:
        print("Restore from {}".format(ckpt_manager.latest_checkpoint))
    else:
        print("Initializing from scratch.")

    start_time = time.time()

    # log information
    out_str  = time.strftime('%d %b %Y %H:%M:%S', time.localtime())
    out_str += '\n'
    out_str += "# (iteration number), T (elapsed seconds), Ltr (training loss), Lge (testing loss)"\
        "AUC, Accuracy, Precision, Recall\n"
    log_name = os.path.join(output_dir, "log_training.txt")
    with open(log_name, 'a') as f:
        f.write(out_str)


    ##====profiling====
    do_profiling = config_tr.get("do_profiling", False)
    if do_profiling:
        
        data_iterator = training_dataset.as_numpy_iterator()
        # computational graphs
        func_log_dir = "logs/{}/funcs".format(time_stamp)
        func_writer = tf.summary.create_file_writer(os.path.join(output_dir, func_log_dir))
        profiling_steps = config_tr.get("profiling_steps", 10)
        profile_logdir = os.path.join(output_dir, "logs/{}/profiling".format(time_stamp))

        tf.summary.trace_on(graph=True, profiler=True)
        for step in range(1):
            inputs_tr, targets_tr = next(data_iterator)
            total_loss = update_step(inputs_tr, targets_tr)[1].numpy()
        with func_writer.as_default():
            tf.summary.trace_export(name='GNN', step=0, profiler_outdir=func_log_dir)

        tf.profiler.experimental.start(profile_logdir)
        for step in range(profiling_steps):
            with tf.profiler.experimental.Trace("train", step_num=step, _r=1):
                inputs_tr, targets_tr = next(data_iterator)
                total_loss = update_step(inputs_tr, targets_tr)[1].numpy()
        tf.profiler.experimental.stop()

        do_profiling_only = config_tr.get("do_profiling_only", False)
        if do_profiling_only:
            exit(0)

        # # memory, cpu usages
        # profile_log_dir = "logs/{}/profiling".format(time_stamp)
        # with tf.profiler.experimental.Profile(profile_log_dir):
        #     tf.profiler.experimental.start()
        #     profiling_steps = config_tr.get("profiling_steps", 10)
        #     data_iterator = training_dataset.as_numpy_iterator()
        #     tf.summary.trace_on(graph=True, profiler=True)
        #     for step in range(profiling_steps):
        #         inputs_tr, targets_tr = next(data_iterator)
                
        #         total_loss = update_step(inputs_tr, targets_tr)[1].numpy()
        #         with func_writer.as_default():
        #             tf.summary.trace_export(name='GNN', step=0, profiler_outdir=func_log_dir)

        #     tf.profiler.experimental.stop()



    writer = tf.summary.create_file_writer(os.path.join(output_dir, "logs/{}/metrics".format(time_stamp)))
    do_profiling = config_tr.get("do_profiling", False)
    if do_profiling:
        # computational graphs
        func_log_dir = "logs/{}/funcs".format(time_stamp)
        func_writer = tf.summary.create_file_writer(os.path.join(output_dir, func_log_dir))
        # profiling for computing usages
        profiling_steps = config_tr.get("profiling_steps", 10)
        profile_logdir = os.path.join(output_dir, "logs/{}/profiling".format(time_stamp))

    previous_metric = 0.0
    threshold = 0.5
    n_fails = 0

    epoch_count = tf.Variable(0, trainable=False, name='epoch_count', dtype=tf.int64)
    for epoch in range(n_epochs):
        total_loss = 0.
        num_batches = 0

        in_list = []
        target_list = []

        for inputs in training_dataset:
            inputs_tr, targets_tr = inputs
            in_list.append(inputs_tr)
            target_list.append(targets_tr)
            if len(in_list) == global_batch_size:
                inputs_tr = utils_tf.concat(in_list, axis=0)
                targets_tr = utils_tf.concat(target_list, axis=0)
                total_loss += update_step(inputs_tr, targets_tr)[1].numpy()
                in_list = []
                target_list = []
                num_batches += 1


        ckpt_manager.save()

        eval_output_name = os.path.join(output_dir, "eval_{}.npz".format(
            ckpt_manager.checkpoint.save_counter.numpy()))

        loss_tr = total_loss/num_batches

        elapsed = time.time() - start_time
        inputs_te_list = []
        target_te_list = []
        predictions = []
        truth_info = []
        num_batches_te = 0
        total_loss_te = 0
        for inputs in testing_dataset:
            inputs_te, targets_te = inputs
            inputs_te_list.append(inputs_te)
            target_te_list.append(targets_te)
            if len(inputs_te_list) == global_batch_size:
                inputs_te = utils_tf.concat(inputs_te_list, axis=0)
                targets_te = utils_tf.concat(target_te_list, axis=0)
                outputs_te = model(inputs_te, num_processing_steps_tr)
                total_loss_te += (tf.math.reduce_sum(
                    loss_fcn(targets_te, outputs_te))/tf.constant(
                        num_processing_steps_tr, dtype=tf.float32)).numpy()
                if loss_name == "GlobalLoss":
                    predictions.append(outputs_te[-1].globals)
                    truth_info.append(targets_te.globals)
                else:
                    predictions.append(outputs_te[-1].edges)
                    truth_info.append(targets_te.edges)
                inputs_te_list = []
                target_te_list = []
                num_batches_te += 1

        loss_te = total_loss_te / num_batches_te
        predictions = np.concatenate(predictions, axis=0)
        truth_info = np.concatenate(truth_info, axis=0)
        # print(tf.math.reduce_sum(predictions).numpy(), tf.math.reduce_sum(truth_info).numpy())

        y_true, y_pred = (truth_info > threshold), (predictions > threshold)
        fpr, tpr, _ = sklearn.metrics.roc_curve(y_true, predictions)
        metric_dict['auc_te'] = sklearn.metrics.auc(fpr, tpr)
        metric_dict['acc_te'] = sklearn.metrics.accuracy_score(y_true, y_pred)
        metric_dict['pre_te'] = sklearn.metrics.precision_score(y_true, y_pred)
        metric_dict['rec_te'] = sklearn.metrics.recall_score(y_true, y_pred)
        metric_dict['loss_te'] = loss_te
        out_str = "* {:05d}, T {:.1f}, Ltr {:.4f}, Lge {loss_te:.4f}, AUC {auc_te:.4f}, A {acc_te:.4f}, P {pre_te:.4f}, R {rec_te:.4f}".format(
            epoch, elapsed, loss_tr, **metric_dict)
        print(out_str)
        with open(log_name, 'a') as f:
            f.write(out_str+"\n")
        np.savez(eval_output_name, predictions=predictions, truth_info=truth_info)

        # save metrics to the summary file
        metric_dict['loss_tr'] = loss_tr
        with writer.as_default():
            for key,val in metric_dict.items():
                tf.summary.scalar(key, val, step=epoch_count)
            writer.flush()
        epoch_count.assign_add(1)

        metric = metric_dict[metric_name]
        if metric < previous_metric:
            print("Current metric {} {:.4f} is lower than previous {:.4f}.".format(metric_name, metric, previous_metric))
            if n_fails < acceptable_fails:
                n_fails += 1
            else:
                print("Reached maximum failure threshold: {} times. Stop Training".format(acceptable_fails))
                break
        else:
            previous_metric = metric