#!/usr/bin/env python

import os

import tensorflow as tf
import numpy as np
import sklearn.metrics

from graph_nets import utils_tf
import sonnet as snt

from root_gnn import model
from root_gnn.utils import load_yaml
from root_gnn.dataset import graph
from root_gnn import utils_plot


ckpt_name = 'checkpoint'

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Evaluate the global classifier")
    add_arg = parser.add_argument
    add_arg("config", help="configuration file that was used for training")
    add_arg("filenames", help="file pattern looking for TFRecord files to be evaluated")
    add_arg("outname", help='output name for some plots')
    add_arg("--modeldir", help="Overwrite the model directory from the configuration file", default=None)
    args = parser.parse_args()

    config = load_yaml(args.config)

    prod_name = config['prod_name']
    modeldir = os.path.join(config['output_dir'], prod_name)
    if args.modeldir is not None:
        modeldir = args.modeldir
    
    config_tr = config['parameters']
    global_batch_size = config_tr['batch_size']
    num_processing_steps_tr = config_tr['n_iters']      ## level of message-passing
    learning_rate = config_tr['learning_rate']

    optimizer = snt.optimizers.Adam(learning_rate)
    model = getattr(model, config['model_name'])()
    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
    ckpt_manager = tf.train.CheckpointManager(checkpoint, directory=modeldir, max_to_keep=5)
    if os.path.exists(os.path.join(modeldir, ckpt_name)):
        status = checkpoint.restore(ckpt_manager.latest_checkpoint)
        print("Loaded latest checkpoint from {}".format(modeldir))
    else:
        raise ValueError("Cannot find model at:", modeldir)

    file_names = tf.io.gfile.glob(args.filenames)
    n_files = len(file_names)
    print("Total {} files".format(n_files))
    
    raw_dataset = tf.data.TFRecordDataset(file_names)
    dataset = raw_dataset.map(graph.parse_tfrec_function)
    AUTO = tf.data.experimental.AUTOTUNE
    dataset = dataset.prefetch(AUTO)

    def create_loss_ops(target_op, output_ops):
        real_weight = fake_weight = 1.0
        weights = target_op.globals * real_weight + (1 - target_op.globals) * fake_weight
        loss_ops = [
            tf.compat.v1.losses.log_loss(target_op.globals, output_op.globals, weights=weights)
            for output_op in output_ops
        ]
        return tf.stack(loss_ops)

    in_list = []
    target_list = []
    predictions = []
    truth_info = []
    num_batches = 0
    total_loss = 0
    for inputs in dataset:
        inputs_tr, targets_tr = inputs
        in_list.append(inputs_tr)
        target_list.append(targets_tr)
        if len(in_list) == global_batch_size:
            inputs_tr = utils_tf.concat(in_list, axis=0)
            targets_tr = utils_tf.concat(target_list, axis=0)
            outputs_tr = model(inputs_tr, num_processing_steps_tr)
            total_loss += (tf.math.reduce_sum(create_loss_ops(targets_tr, outputs_tr)) / tf.constant(num_processing_steps_tr, dtype=tf.float32)).numpy()
            predictions.append(outputs_tr[-1].globals)
            truth_info.append(targets_tr.globals)
            in_list = []
            target_list = []
            num_batches += 1
    print("Total {} batches with batch size of {}".format(num_batches, global_batch_size))
    print("Averaged Loss:", total_loss/num_batches)
    predictions = np.concatenate(predictions, axis=0)
    truth_info = np.concatenate(truth_info, axis=0)
    
    # these keywords are hardcoded, used by calculate_merits
    np.savez(args.outname+".npz", predictions=predictions, truth_info=truth_info)

    utils_plot.plot_metrics(predictions, truth_info, outname=args.outname+".pdf", off_interactive=True)